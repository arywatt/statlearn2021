---
title: "Hw_2"
author: ""
date: "5/25/2021"
output: html_document
---

```{r setup, include=FALSE}
library(caret)
library(sigmoid)
library(sets)
library(cleanNLP)
library(tm)
```

## Part 1 : Process the dataset

### Load the data 
```{r }
data = load("amazon_review_clean.RData")
```

### Check data size 
```{r}
dim(X_tr)
dim(X_te)
length(y_tr)
length(y_te)
```

```{r}
labels_te = as.numeric(factor(y_te,levels = c('book','movies'),labels = c(0,1)))
labels_tr = as.numeric(factor(y_tr,levels = c('book','movies'),labels = c(0,1)))

```

### Check and filter features 
#### stop words
```{r}

#cnlp_download_spacy(model_name = "en")
#nlp_proc = cnlp_init_spacy("en")
stop_words = stopwords("en")
#stop_words

```

#### Identify useless features 
```{r,error=T,warning=FALSE}

 features <- colnames(X_tr)
 features_to_use = c()
 
 for (feature in features){
   # check if feature is non numeric 
   is_str = is.character(feature)  
   
   # check if non numeric 
   is_non_numeric = is.na(as.numeric(feature))
   
   #check if not a punctuation
   is_not_punctuation = nchar(feature) > 1
   
   ## check if not a stop-word
   is_not_stopword = ! feature %in% stop_words
   
   # We retain only eature which pass all checks
   pass = is_str & is_non_numeric & is_not_punctuation & is_not_stopword
   if (pass) {
     features_to_use = append(features_to_use, feature)
   }
 }
 
 length(features_to_use)
```


### Final data set with  selected features 

```{r}
#tf_tr = X_tr[,features_to_use]
tf_te = X_te[,features_to_use]
```

```{r}
#dim(clean_x_tr)
dim(tf_te)
```

### Build tf-idf matrix 

#### IDF matrices
```{r}
# For test matrices

# idf_te <- log( nrow(tf_te) / ( 1 + colSums(tf_te != 0) ) )
# idf_te_diag <- diag(idf_te)
# dim(idf_te_diag)

```

#### TF-IDF matrices

```{r}
# tf_idf <- tf_te %*% idf_te_diag
# colnames(tf_idf) <- colnames(tf_te)
# dim(tf_idf)
```
#### TF-IDF Matrix normalization

```{r}
tf_idf = tf_te / sqrt( rowSums( tf_te^2 ) )
```

## Part 2

### Gradient descent algorithm

```{r}
# x is an inputs x features matrix
# y is an inputs x 1 matrix for the labels 
# alpha: This is the learning rate of the algorithm.
# epsilon : arrest condition 

GradD <- function(x, y, size= 1, alpha = 0.006, iters = 1000,epsilon = 0.0001){
  iter <- 0
  i <- 0
  x <- cbind(rep(1,nrow(x)), x)
  N = nrow(x)
  theta <-  as.matrix(rep(0,ncol(x))
  
  # Loss function -  binary entropy loss 
  cost <- function(x,y,theta){
    if (size > 1) y_pred <- sigmoid(x %*% theta)
    if (size == 1) y_pred <- sigmoid(t(x) %*% theta)
    return ((-1/nrow(x)) * sum( log(y_pred)*y + (1 - y)*log(1 - y_pred) ))
  }
  
  # Theta init 
  samples = sample(x = 1:N, size = size)
  x_loop = as.matrix(x[samples,])
  y_loop = y[samples]
  costs = c(cost(x_loop,y_loop,theta))
  delta <- 1
  
  ## Loop 
  for (i in 2:iters){
    samples = sample(x = 1:N, size = size)
    x_loop = as.matrix(x[samples,])
    #x_loop = x[samples,]
    y_loop = y[samples]
    
    if (size > 1) y_pred <- sigmoid(x_loop %*% theta)
    if (size == 1) y_pred <- sigmoid(t(x_loop) %*% theta)
    
    
    # print(length(y_pred))
    # print(length(y_loop))
    # print(length(x_loop))
    # 
    
    if (size > 1) grad = (t(x_loop) %*% (y_pred - y_loop))
    if (size == 1) grad = x_loop %*% (y_pred - y_loop)
    theta <- theta - (alpha / size) * grad
    cval <- cost(x_loop,y_loop,theta)
    
    if(i%%100==0) print(paste("iter ",i," : ",cval))
    
    costs <- append(costs, cval)
    delta <- abs(costs[i] - costs[i-1])
    print(delta)
    if(delta <= epsilon) break
    if(costs[i] - costs[i-1] > 0){
      print("The cost is increasing.  Try reducing alpha.")
      #return()
    }
    iter <- append(iter, i)
   
    if(i == iters +1) break
  }
  print(sprintf("Completed in %i iterations.", i))
  print(paste("Final gradient norm is",sqrt(sum(grad^2))))
  return(theta)
}


# Function to predict 
  #y_te=ifelse(y_te=="book",1,0)
TPredict <- function(theta, x){
  x <- cbind(rep(1,nrow(x)), x)
  y_pred=x %*% theta
  y_pred=ifelse(y_pred>0,1,0)
  return(table(y_pred,y_te))
  }


```


```{r}
# gradientR<-function(X,y, epsilon=0.0001,eta=0.001, iters=100){
#       
#       X = as.matrix(data.frame(rep(1,length(y)),X))
#       N= nrow(X)
#       print("Initialize parameters...")
#       theta.init = as.matrix(rnorm(n=ncol(X), mean=0,sd = 1)) # Initialize theta
#       theta.init = t(theta.init)
#        e = t(y) - theta.init%*%t(X)
#        grad.init = -(2/N)%*%(e)%*%X
#        theta = theta.init - eta*(1/N)*grad.init
#        l2loss = c()
#       for(i in 1:iters){
#           l2loss = c(l2loss,sqrt(sum((t(y) - theta%*%t(X))^2)))
#           e = t(y) - theta%*%t(X)
#           grad = -(2/N)%*%e%*%X
#           theta = theta - eta*(2/N)*grad
#             if(sqrt(sum(grad^2)) <= epsilon){
#               break
#             }
#         }
#   print("Algorithm converged")
#   print(paste("Final gradient norm is",sqrt(sum(grad^2))))
#   values<-list("coef" = t(theta), "l2loss" = l2loss)
#   return(values)
# }
```


#### Apply GD 

```{r}
n = 1000
# x = tf_idf[samples,]
# y = labels_te[samples]

x = tf_idf
y = labels_te

# thet = gradientR(x,y,iters = 1000)
thet2 = GradD(x,y,size=2,iters = 10,epsilon = 0.001)
```



```{r}
#pred1 = sigmoid(x %*% thet$coef)
pred2 = sigmoid(x %*% thet2)

```


```{r}
summary(pred2)
```

```{r}
final_pred = pred2
summary(final_pred)


```


```{r}
final_pred[pred2>=0.5] = "book"
final_pred[pred2<0.5] = "movie"
```

```{r}
table(final_pred)
```

```{r}
table(final_pred,y_te)
```

