---
title: "Statlearn HW_1"
author: "xxx; yyyyy; zzzzz; hhhhhh; vvvvvv"
date: "15/03/2021"
output: html_document
header-includes :
- \usepackage { amsmath }
---


# SL - Homework 1

## Libraries 

```{r}
# Here we include all libraires needed to run 
library('caret')
library('viridis')

```


## Excercise 1

Polynomial regression is one example of regression analysis using basis functions to model a functional relationship between two quantities. More specifically, it replaces {\displaystyle x\in \mathbb {R} ^{d_{x}}}{\displaystyle x\in \mathbb {R} ^{d_{x}}} in linear regression with polynomial basis {\displaystyle \varphi (x)\in \mathbb {R} ^{d_{\varphi }}}{\displaystyle \varphi (x)\in \mathbb {R} ^{d_{\varphi }}}, e.g. {\displaystyle [1,x]{\mathbin {\stackrel {\varphi }{\rightarrow }}}[1,x,x^{2},\ldots ,x^{d}]}{\displaystyle [1,x]{\mathbin {\stackrel {\varphi }{\rightarrow }}}[1,x,x^{2},\ldots ,x^{d}]}. A drawback of polynomial bases is that the basis functions are "non-local", meaning that the fitted value of y at a given value x = x0 depends strongly on data values with x far from x0.[5] In modern statistics, polynomial basis-functions are used along with new basis functions, such as splines, radial basis functions, and wavelets. These families of basis functions offer a more parsimonious fit for many types of data.



```{r}
ff <- function(x,j){
  f= exp(j*x)
  if (j!=0){
    f = (j%%2 ==0)* exp(2*j*x) + (j%%2 !=0)*exp(-2*j*x)
  }
return(f)
}
```


```{r}
# Lets define our cosine functions 
# Cosine-basis
cos.basis = function(x, j = 4)  1*(j == 0) + sqrt(2)*cos(pi*j*x)*(j > 0)


# Plot the first 10 basis functions
j.max = 6
mycol = viridis(j.max + 1, alpha = .5)


# Open the graphical device
curve(cos.basis(x,0), n = 501,  ylim = c(-2,2), col = mycol[1], lwd = 3,
      main = " ", xlab = "", ylab = expression(phi[j](x)))


# Add the other basis functions
for (idx in 1:j.max) curve(cos.basis(x, j = idx), n = 501, add = T, col = mycol[idx + 1], lwd = 3)
legend("bottom", paste("j =", 0:j.max), col = mycol, lwd = 4, cex = .7, bty = "n", horiz = T)


```
### lets compute the doppler funtion and estimate coefficients 

```{r}

# Doppler function scaled in [0,1]
doppler.fun <-  function(x) sqrt(x*(1 - x))*sin( (2.1*pi)/(x + 0.05) )
curve(doppler.fun(x), from = 0, to = 1, main = "", xlab = "", ylab = "m(x)",
      n = 1001, col = gray(.8), lwd = 3)

# Part 3 ------------------------------------------------------------------

# Let's now numerically evaluate the Fourier coefficients 
# of the Doppler under our cosine-basis
j.max   <- 200
f.coeff <- rep(NA, j.max+1)
for (idx in 0:j.max){
  foo = tryCatch(
    integrate(function(x, j) doppler.fun(x) * cos.basis(x,j), lower = 0, upper = 1, j = idx)$value,
    error = function(e) NA
  )
  f.coeff[idx + 1] = foo
}
# Visualize the Fourier coefficients
plot(f.coeff, type = "h", ylab = expression(beta[j]), main = "", xlab = "")


```
```{r}
#Now we order the coefficients in descending order w.r.t to their absolute value
ord = order(abs(f.coeff), decreasing = T) # indexes from max to  min abslute value

# Visualize the Fourier coefficients
nf.coeff = f.coeff[ord]
plot(nf.coeff, type = "h", ylab = expression(beta[j]), main = "", xlab = "")
```


### Rebuild the appromating function 

```{r}
# Time to rebuild/approximate our Doppler with an n-term (linear) approximation.
# Let's make a function for this purpose...

# Linear approximation
proj.cos <- function(x, f.coeff, j.max = 10){
  out = rep(0, length(x))
  for(idx in 0:j.max){
    if ( !is.na(f.coeff[idx + 1]) ) out = out + f.coeff[idx + 1] * cos.basis(x, j = idx)
  }
  return(out)
}

# Non linear approximation
proj.cos.nl <- function(x, f.coeff, j.max = 10){
  out = rep(0, length(x))
  for(idx in 0:j.max){
    ord.id = ord[idx + 1] # We retrieve the index of the greatest (idx+1)-th greatest coefficient in absolute value
    if ( !is.na(f.coeff[ord.id]) ) {
      out = out +   f.coeff[ord.id] * cos.basis(x, j = ord.id -1) # apply the basis function retrieving coeff at right index
    }
  }
  return(out)
}

```

### Compute errors of approximations

```{r}
mse <- function(mx,mjx, n_terms){
  er = NA
  fo=tryCatch(
    integrate(function(x,j) (mx(x)- mjx(x, f.coeff,n_terms))^2, lower = 0, upper = 1)$value, error = function(e) NA )
    er = sqrt(fo)
  return (er)
}


```



### Visualize approximations

```{r}
# Visualize some n-terms approximations
j.seq =  c(5, 50, 150)
mycol = viridis(length(j.seq), alpha = .7)

par(mfrow = c(3,2)) # split the graphical device in a 2 x 3 matrix
for (idx in 1:length(j.seq)){
  # Original function
  curve(doppler.fun(x), from = 0, to = 1, 
        main = paste(j.seq[idx], "-term  Linear approximation", sep = ""),
        xlab = paste('error ', mse(doppler.fun, proj.cos,j.seq[idx])), 
                     ylab = expression(m[J](x)),
        n = 1001, col = gray(.8), lwd = 3)
  # Add approximation
  curve(proj.cos(x, f.coeff = f.coeff, j.seq[idx]),
        n = 1001, col = mycol[idx], lwd = 4,
        add = TRUE)
  
   curve(doppler.fun(x), from = 0, to = 1, 
        main = paste(j.seq[idx], "-term  Non linear approximation", sep = ""),
        xlab = paste('error ', mse(doppler.fun, proj.cos.nl,j.seq[idx]))
        , ylab = expression(m[J](x)),
        n = 1001, col = gray(.8), lwd = 3)
   
  curve(proj.cos.nl(x, f.coeff = f.coeff, j.seq[idx]),
        n = 1001, col = mycol[idx], lwd = 4,
        add = T)
}
par(mfrow = c(1,3)) # back to default




```





## Excercise 2

```{r}

##used in order to generate those functions such that gi(x)=x^i-1 (the blue functions)
g_b=function(x,d){
  gx=x^(d-1)
  return(gx)
}

##plotting the power functions together
plot_power = function(d){
  par(mfrow=c(1,1))
  curve(g_b(x,1), main="power functions", from = -2, to=2, ylim=c(-0.5, 10), col="darksalmon")
  for (i in 2:(d+1)) {
    curve(g_b(x,i), add=T, col=i)
  }
}

```



```{r}
##generates the truncated power functions (brown functions)

trunc=function(x, j, d, q) { #j is the jth knot considered, d is the degree of the polinomial, q is the number of knots
  knots= quantile((0:1), probs = seq(0,1,length.out=q)) 
  #generates q equispaced knots in (0,1)
  knot=knots[j] #considers the jth knot 
  ifelse(x- knot>0, ((x-knot)^d), 0)} 
#if the difference between x and the jth knot is greater than 0, then the function has value (x - jth knot)^d, otherwise it's truncated at zero


##plots the truncated fncs separately, one graph each
plot_trunc_sep=function(q,d){ 
par(mfrow = c(2,q/2))
for (j in 1:q) {
        curve(trunc(x, j, d, q), from = -2, to=2, col="red", main = "truncated power fnc")
  }
}  


```


```{r}
#plots the truncated power functions on same page
plot_trunc_tog=function(q,d,from, to){ 
  #par(mfrow = c(1,1))
  curve(trunc(x, 1, d, q), from = from, to=to, col="red", main = "truncated power fnc")
  for (j in 2:q) {
    curve(trunc(x, j, d, q), col=j, main = "truncated power fnc", add=T)
    
  }
}




##1st combination of d=1, q=3
plot_power(1)
plot_trunc_tog(3,1, -1, 2)


```


```{r}

curve(g_b(x,1), main="power functions", col="yellow", from = -2, to=2, ylim=c(-0.5, 2))  ##g1 = x^0 constant
curve(g_b(x,2), add=T, col="pink")  ##g2 = x^1 bisector    
curve(trunc(x, 1, 1, 3), col="red", add = T)
curve(trunc(x, 2, 1, 3), from = -2, to=2, col="blue",  add = T)
curve(trunc(x, 3, 1, 3), from = -2, to=2, col="green", add=T)

```


```{r}
##2nd combination of d=3, q=5
plot_power(3)
plot_trunc_tog(5,3, -0.5,5 )

curve(g_b(x,1), main="constant fnc", col="yellow", from = -2, to=2, ylim=c(-0.5, 5))  ##g1 = x^0 constant
for (i in 2:4 ) {
  curve(g_b(x,i), add=T)
  }


```


```{r}

#3rd combination d=5, q=10
# plot_power(d=5)
# plot_trunc_tog(10,5)

```

### Ex. 2.3

```{r}
load(file = 'resources/ieri_domani.Rdata')
```

```{r design matrix}
obs=df$x
#the generic entry of the design matrix X in i,j is the value of the jth fnc computed for the ith unit of the observations vector x, X[i, j] = gj(xi)
#We want to study this matrix for 3 different values of q, i.e. (3,5,10) with 
#poly degree d fixed at d=3


knots=function(q){return(quantile(obs,probs = seq(0,1,length.out = q)))}

design=function(q,d){
  
  values = matrix(NA,nrow= length(obs) , ncol =q+d+1)
  knots=knots(q)
  numcol = q+d+1
  
  for(j in 1:numcol){
    knot=ifelse(j<= d+1, 0, knots[j-d-1])
    values[,j] = (j <=d+1 )* obs^(j-1) + (j >d+1 )*(obs - knot)^3
    values=ifelse(values>0,values,0)
  }
  return(values)
}

```

```{r spline regression}

X_3_3=cbind(design(3,3), df$y.yesterday) 

X_3_3=as.data.frame(X_3_3)
#linear regression on the design matrix
fit_3_3=lm(V8 ~ V1+ V2+ V3 + V4 + V5 + V6 + V7, data=X_3_3) 
fit_3_3$coefficients  ##na coefficients are the 1st one the 5th one and the 7th one
summary(fit_3_3)



X_5_3=cbind(design(5,3), df$y.yesterday)
X_5_3=as.data.frame(X_5_3)
fit_5_3=lm(V10 ~ V1+ V2+ V3 + V4 + V5 + V6 + V7 + V8 + V9, data=X_5_3)
fit_5_3$coefficients  ## na coefficients are the 1st (all ones) one the fifth one and the last one (all zero)
summary(fit_5_3)

head(design(10,3))

X_10_3=cbind(design(10,3), df$y.yesterday)
X_10_3=as.data.frame(X_10_3)
fit_10_3=lm(V15 ~ V1+ V2+ V3 + V4 + V5 + V6 + V7 + V8 + V9+ V10 + V11+ V12+ V13+ V14, data=X_10_3)
fit_10_3$coefficients  ## na coefficients are the 1st (all ones) one the fifth one and the last one (all zero)
summary(fit_10_3)
```



```{r cp & cv}
#### Mallow's CP
##defining the MSE function
MSE_tr=function(residuals){
  mse=1/length(residuals)*(sum((residuals)^2))
  return(mse)
  }

MSE_tr(fit_3_3$residuals)

##Mallows' cp estimation
#given the tot # of parameters ps and the residuals of a fitted model, returns the Mallows' Cp value
Cp=function(ps, residuals){ 
  n=length(residuals)
  MSE.tr=MSE_tr(residuals)
  hatsigma2 <- ( n*MSE.tr ) / (n - ps)
  Cps <- MSE.tr + (2 * hatsigma2 * ps) / n
  return(Cps)
}

##q=3 d=3 model
cp_3_3=Cp(8, fit_3_3$residuals)


##q=5 d=3 model
cp_5_3=Cp(10, fit_5_3$residuals)


##q=10 d=3 model
cp_10_3=Cp(15, fit_10_3$residuals)

which.min(c(cp_3_3, cp_5_3, cp_10_3)) ##the best model according to the evaluation of its mallows' cp is the one with 10 knots

##generalized cross validation  

ps=c(length(fit_3_3$coefficients), length(fit_5_3$coefficients), length(fit_10_3$coefficients))                                      
GCV= MSEs.tr / (1 - (ps)/n )^2

plot(c(3,5,10), GCV, type = "b", xlab = "q")

###k fold cross validation
K = 5
set.seed(123) # reproducibility
require(boot) # load the package

##five fold cross validation for the model d=3 q=3
KCV_3_3 = cv.glm(X_3_3, glm(V8 ~ V1+ V2+ V3 + V4 + V5 + V6 + V7, X_3_3, family =gaussian), K = K )$delta[1]


##five fold cross validation for the model d=3 q=5
KCV_5_3 = cv.glm(X_5_3, glm(V10 ~ V1+ V2+ V3 + V4 + V5 + V6 + V7 + V8 + V9, X_5_3, family =gaussian), K = K )$delta[1]



##five fold cross validation for the model d=3 q=10
KCV_10_3 = cv.glm(X_10_3, glm(V15 ~ V1+ V2+ V3 + V4 + V5 + V6 + V7 + V8 + V9+ V10 + V11+ V12+ V13+ V14, X_10_3, family =gaussian), K = K )$delta[1]


KCV=c(KCV_3_3, KCV_5_3, KCV_10_3)
plot(c(3,5,10), KCV, type = "b", log = "y", xlab = "q")
```



```{r 2.4 comparison}
pol5_fit=lm(y.yesterday ~ poly(x, degree = 5), df) #best polinomial

#comparison
AIC(fit_10_3, pol5_fit)
###even if the spline has more parameters, 13, than the polinomial, the AIC suggest the usage of the spline as it minimizes the loss value

cp_pol5=Cp(ps=length(pol5_fit$coefficients), residuals=pol5_fit$residuals)

which.min(c(cp_10_3, cp_pol5))

#For Linear Models: Cp and AIC are proportional: the lowest Cp corresponds to the lowest AIC. AIC attests what we already know from Cp value, the best model between the two is the spline.
```
